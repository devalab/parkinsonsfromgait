{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shanmukh.alle/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shanmukh.alle/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shanmukh.alle/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shanmukh.alle/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shanmukh.alle/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shanmukh.alle/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import *\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.activations import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n",
    "from tensorflow.keras import layers, optimizers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "tf.keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 100\n",
    "path = \"../../dataset/processed100/force/\"\n",
    "# path = \"../dataset/processed50/lp_residual/\"\n",
    "# path = \"./dataset/raw/\"\n",
    "modelstore = \"/scratch/shanmukh.alle/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Walk Name</th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>HoehnYahr</th>\n",
       "      <th>UPDRS</th>\n",
       "      <th>UPDRSM</th>\n",
       "      <th>TUAG</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Extra Task</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaCo01_01.txt</td>\n",
       "      <td>GaCo01</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>1.80</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.075</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GaCo02_01.txt</td>\n",
       "      <td>GaCo02</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>1.74</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.040</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GaCo02_02.txt</td>\n",
       "      <td>GaCo02</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>1.74</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.162</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GaCo03_01.txt</td>\n",
       "      <td>GaCo03</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>1.80</td>\n",
       "      <td>101.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.051</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GaCo03_02.txt</td>\n",
       "      <td>GaCo03</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>1.80</td>\n",
       "      <td>101.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.265</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>SiPt36_01.txt</td>\n",
       "      <td>SiPt36</td>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>1.58</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>11.27</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>SiPt37_01.txt</td>\n",
       "      <td>SiPt37</td>\n",
       "      <td>2</td>\n",
       "      <td>66</td>\n",
       "      <td>1.70</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>27.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7.56</td>\n",
       "      <td>1.010</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>SiPt38_01.txt</td>\n",
       "      <td>SiPt38</td>\n",
       "      <td>2</td>\n",
       "      <td>65</td>\n",
       "      <td>1.59</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.13</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>SiPt39_01.txt</td>\n",
       "      <td>SiPt39</td>\n",
       "      <td>2</td>\n",
       "      <td>69</td>\n",
       "      <td>1.68</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.97</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>SiPt40_01.txt</td>\n",
       "      <td>SiPt40</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>1.60</td>\n",
       "      <td>81.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>37.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>9.70</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>306 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Walk Name Patient ID  Gender  Age  Height  Weight  HoehnYahr  UPDRS  \\\n",
       "0    GaCo01_01.txt     GaCo01       1   66    1.80    83.0        0.0    0.0   \n",
       "1    GaCo02_01.txt     GaCo02       1   74    1.74    70.0        0.0    1.0   \n",
       "2    GaCo02_02.txt     GaCo02       1   74    1.74    70.0        0.0    1.0   \n",
       "3    GaCo03_01.txt     GaCo03       1   69    1.80   101.0        0.0    0.0   \n",
       "4    GaCo03_02.txt     GaCo03       1   69    1.80   101.0        0.0    0.0   \n",
       "..             ...        ...     ...  ...     ...     ...        ...    ...   \n",
       "301  SiPt36_01.txt     SiPt36       2   53    1.58    62.0        2.0   52.0   \n",
       "302  SiPt37_01.txt     SiPt37       2   66    1.70    62.0        2.5   27.0   \n",
       "303  SiPt38_01.txt     SiPt38       2   65    1.59    60.0        2.0   22.0   \n",
       "304  SiPt39_01.txt     SiPt39       2   69    1.68    53.0        2.0   33.0   \n",
       "305  SiPt40_01.txt     SiPt40       1   69    1.60    81.0        2.5   37.0   \n",
       "\n",
       "     UPDRSM   TUAG  Speed  Extra Task  Class  \n",
       "0       0.0    NaN  1.075           0      0  \n",
       "1       1.0    NaN  1.040           0      0  \n",
       "2       1.0    NaN  1.162           0      0  \n",
       "3       0.0    NaN  1.051           0      0  \n",
       "4       0.0    NaN  1.265           0      0  \n",
       "..      ...    ...    ...         ...    ...  \n",
       "301    32.0  11.27  0.970           0      1  \n",
       "302    21.0   7.56  1.010           0      1  \n",
       "303    14.0  10.13  1.070           0      1  \n",
       "304    20.0  13.97  0.880           0      1  \n",
       "305    24.0   9.70  1.070           0      1  \n",
       "\n",
       "[306 rows x 13 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walk_df = pd.read_csv(\"../../dataset/WalksDemographics.csv\")\n",
    "walk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "306it [00:38,  8.05it/s]\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "for i in set(walk_df[\"Patient ID\"]):\n",
    "    data[i] = []\n",
    "    \n",
    "for index,row in tqdm.tqdm(walk_df.iterrows()):\n",
    "    patient = row[\"Patient ID\"]\n",
    "#     features = row[[\"Gender\",\"Age\",\"Height\",\"Weight\",\"TUAG\",\"Speed\",\"Extra Task\"]]\n",
    "    features = row[[\"Gender\",\"Age\",\"Height\",\"Weight\",\"TUAG\"]]\n",
    "\n",
    "    walk_name = row[\"Walk Name\"]\n",
    "    walk_seq = np.loadtxt(path+walk_name,delimiter=\",\")\n",
    "    sample = (walk_seq,features,row[\"Class\"])\n",
    "    data[patient].append(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12119, 18)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walk_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = sorted(list(data.keys()))\n",
    "labels = []\n",
    "for i in patients:\n",
    "    labels.append(walk_df[walk_df[\"Patient ID\"]==i].iloc[0][\"Class\"])\n",
    "#     print (i,labels[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def window(samples,feature,label,cut_length=100):\n",
    "    inputs = []\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(len(samples)):\n",
    "        sample = samples[i]\n",
    "        cut = int(cut_length/2)\n",
    "        for j in range(int(len(sample)/cut)):\n",
    "            if (j+2)*cut>=len(sample):\n",
    "                break\n",
    "            inputs.append(sample[j*cut:(j+2)*cut,:])\n",
    "            features.append(feature[i])\n",
    "            labels.append(label[i])\n",
    "            \n",
    "    inputs = np.stack(inputs)\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return inputs, features, labels\n",
    "\n",
    "def pad(samples):\n",
    "    lengths = [len(i) for i in samples]\n",
    "    max_len = max(lengths)\n",
    "    for i in range(len(samples)):\n",
    "        pad_len = max_len - lengths[i]\n",
    "        samples[i] = np.pad(samples[i],((0,pad_len),(0,0)),\"wrap\")\n",
    "    return np.stack(samples)\n",
    "    \n",
    "def get_from_dict(dictionary, keys):\n",
    "    output = []\n",
    "    for i in keys:\n",
    "        output += dictionary[i]\n",
    "    return output\n",
    "\n",
    "def get_best_model(path):\n",
    "    models = os.listdir(path)\n",
    "    accuracy = {}\n",
    "    for i in models:\n",
    "        info = i.split(\"-\")\n",
    "        try:\n",
    "            accuracy[float(info[-1][:-5])][float(info[-2])] = i\n",
    "        except:\n",
    "            accuracy[float(info[-1][:-5])] = {}\n",
    "            accuracy[float(info[-1][:-5])][float(info[-2])] = i\n",
    "    best_acc = max(accuracy.keys())\n",
    "    best_loss = min(accuracy[best_acc].keys())\n",
    "    model_path = accuracy[best_acc][best_loss]\n",
    "    \n",
    "    model = tf.keras.models.load_model(path+\"/\"+model_path)\n",
    "    return model\n",
    "\n",
    "class VariancePooling(tf.keras.layers.Layer):\n",
    "    def __init__(self, ):\n",
    "        super(VariancePooling, self).__init__()\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.math.reduce_std(x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1D_full():\n",
    "    '''\n",
    "    :return: 1 branch of the parallel Convnet\n",
    "    '''\n",
    "    input1 = Input(shape=(100, 1))\n",
    "    x = Conv1D(filters=8, kernel_size=3, activation='selu', padding='valid')(input1)\n",
    "    x = Conv1D(filters=16, kernel_size=3, activation='selu', padding='valid')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = Conv1D(filters=16, kernel_size=3, activation='selu', padding='valid')(x)\n",
    "    x = Conv1D(filters=16, kernel_size=3, activation='selu', padding='valid')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(50, activation='elu')(x)\n",
    "    model = Model(input1, x)\n",
    "    rms = optimizers.RMSprop(lr=0.001, decay=0.005)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])\n",
    "#     print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def multiple_cnn1D(nb=18):\n",
    "    '''\n",
    "    :param nb: number of features ( indicates the number of parallel branches)\n",
    "    :return:\n",
    "    '''\n",
    "    inputs = Input(shape=(100,nb))\n",
    "    outputs = []\n",
    "    for i in range(nb):\n",
    "        x = inputs[:,:,i]\n",
    "        model = conv1D_full()\n",
    "        x = tf.expand_dims(x,axis=-1)\n",
    "        x = model(x)\n",
    "        outputs.append(x)\n",
    "    x = concatenate(outputs,axis=-1)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = layers.Dense(100, activation='selu')(x)\n",
    "    x =  Dropout(0.5)(x)\n",
    "    x = layers.Dense(20, activation='selu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    answer = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, answer)\n",
    "    opt = optimizers.RMSprop(lr=0.001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "#     print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shanmukh.alle/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/shanmukh.alle/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py:4277: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "['GaCo07', 'GaPt06', 'GaPt08', 'GaPt12', 'GaPt26', 'JuCo02', 'JuCo05', 'JuCo08', 'JuCo25', 'JuPt08', 'JuPt19', 'SiCo01', 'SiCo16', 'SiCo21', 'SiPt28', 'SiPt31', 'SiPt39']\n",
      "4718/4718 [==============================] - 3s 701us/sample - loss: 0.8087 - acc: 0.5570\n",
      "4718/4718 [==============================] - 1s 286us/sample - loss: 0.8087 - acc: 0.5570\n",
      "Train on 61197 samples, validate on 4718 samples\n",
      "Epoch 1/1000\n",
      "61197/61197 [==============================] - 10s 164us/sample - loss: 0.9315 - acc: 0.6298 - val_loss: 0.6132 - val_acc: 0.6628\n",
      "Epoch 2/1000\n",
      "61197/61197 [==============================] - 4s 60us/sample - loss: 0.6359 - acc: 0.6901 - val_loss: 0.6945 - val_acc: 0.6492\n",
      "Epoch 3/1000\n",
      "61197/61197 [==============================] - 4s 60us/sample - loss: 0.5766 - acc: 0.7224 - val_loss: 0.8942 - val_acc: 0.6308\n",
      "Epoch 4/1000\n",
      "61197/61197 [==============================] - 4s 60us/sample - loss: 0.5435 - acc: 0.7436 - val_loss: 0.5283 - val_acc: 0.7868\n",
      "Epoch 5/1000\n",
      "61197/61197 [==============================] - 4s 61us/sample - loss: 0.5159 - acc: 0.7639 - val_loss: 0.6166 - val_acc: 0.6978\n",
      "Epoch 6/1000\n",
      "61197/61197 [==============================] - 4s 60us/sample - loss: 0.4976 - acc: 0.7764 - val_loss: 0.5850 - val_acc: 0.7251\n",
      "Epoch 7/1000\n",
      "61197/61197 [==============================] - 4s 60us/sample - loss: 0.4819 - acc: 0.7875 - val_loss: 0.5181 - val_acc: 0.7654\n",
      "Epoch 8/1000\n",
      "61197/61197 [==============================] - 4s 60us/sample - loss: 0.4648 - acc: 0.7998 - val_loss: 0.5660 - val_acc: 0.7355\n",
      "Epoch 9/1000\n",
      "61197/61197 [==============================] - 4s 60us/sample - loss: 0.4481 - acc: 0.8106 - val_loss: 0.6002 - val_acc: 0.7151\n",
      "Epoch 10/1000\n",
      "61197/61197 [==============================] - 4s 60us/sample - loss: 0.4400 - acc: 0.8152 - val_loss: 0.5693 - val_acc: 0.7365\n",
      "Epoch 11/1000\n",
      "61197/61197 [==============================] - 4s 61us/sample - loss: 0.4279 - acc: 0.8202 - val_loss: 0.6115 - val_acc: 0.7217\n",
      "Epoch 12/1000\n",
      "61197/61197 [==============================] - 4s 61us/sample - loss: 0.4149 - acc: 0.8299 - val_loss: 0.5900 - val_acc: 0.7493\n",
      "Epoch 13/1000\n",
      "61197/61197 [==============================] - 4s 60us/sample - loss: 0.3975 - acc: 0.8374 - val_loss: 0.5641 - val_acc: 0.7158\n",
      "Epoch 14/1000\n",
      "61197/61197 [==============================] - 4s 60us/sample - loss: 0.3844 - acc: 0.8424 - val_loss: 0.6508 - val_acc: 0.7242\n",
      "Epoch 15/1000\n",
      "61197/61197 [==============================] - 4s 60us/sample - loss: 0.3697 - acc: 0.8505 - val_loss: 0.7192 - val_acc: 0.6859\n",
      "Epoch 16/1000\n",
      "61197/61197 [==============================] - 4s 61us/sample - loss: 0.3526 - acc: 0.8578 - val_loss: 0.8679 - val_acc: 0.6831\n",
      "Epoch 17/1000\n",
      "61197/61197 [==============================] - 4s 60us/sample - loss: 0.3369 - acc: 0.8658 - val_loss: 0.6960 - val_acc: 0.6969\n",
      "Epoch 18/1000\n",
      "61197/61197 [==============================] - 4s 60us/sample - loss: 0.3217 - acc: 0.8713 - val_loss: 0.9347 - val_acc: 0.6630\n",
      "Epoch 19/1000\n",
      "61197/61197 [==============================] - 4s 60us/sample - loss: 0.3043 - acc: 0.8786 - val_loss: 0.8710 - val_acc: 0.6842\n",
      "Epoch 20/1000\n",
      "61197/61197 [==============================] - 4s 61us/sample - loss: 0.2866 - acc: 0.8872 - val_loss: 1.0558 - val_acc: 0.6649\n",
      "Epoch 21/1000\n",
      "60000/61197 [============================>.] - ETA: 0s - loss: 0.2743 - acc: 0.8899\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "61197/61197 [==============================] - 4s 69us/sample - loss: 0.2738 - acc: 0.8903 - val_loss: 1.2444 - val_acc: 0.6564\n",
      "Epoch 22/1000\n",
      "61197/61197 [==============================] - 4s 61us/sample - loss: 0.2386 - acc: 0.9067 - val_loss: 1.1618 - val_acc: 0.7194\n",
      "Epoch 23/1000\n",
      "61197/61197 [==============================] - 4s 60us/sample - loss: 0.2299 - acc: 0.9108 - val_loss: 1.2131 - val_acc: 0.7103\n",
      "Epoch 24/1000\n",
      "61197/61197 [==============================] - 4s 60us/sample - loss: 0.2231 - acc: 0.9141 - val_loss: 1.2442 - val_acc: 0.6958\n",
      "Epoch 25/1000\n",
      "61197/61197 [==============================] - 4s 60us/sample - loss: 0.2160 - acc: 0.9155 - val_loss: 1.3767 - val_acc: 0.6956\n",
      "Epoch 26/1000\n",
      "61197/61197 [==============================] - 4s 65us/sample - loss: 0.2077 - acc: 0.9200 - val_loss: 1.1691 - val_acc: 0.7022\n",
      "Epoch 27/1000\n",
      "60800/61197 [============================>.] - ETA: 0s - loss: 0.2045 - acc: 0.9209Restoring model weights from the end of the best epoch.\n",
      "61197/61197 [==============================] - 4s 63us/sample - loss: 0.2045 - acc: 0.9209 - val_loss: 1.1923 - val_acc: 0.6992\n",
      "Epoch 00027: early stopping\n",
      "0.8974358974358975 0.8636363636363636 0.896551724137931\n",
      "['JuCo07', 'JuCo12', 'JuCo18', 'JuCo26', 'JuPt02', 'JuPt05', 'JuPt11', 'JuPt23', 'JuPt25', 'SiCo06', 'SiCo07', 'SiCo18', 'SiCo25', 'SiPt20', 'SiPt24', 'SiPt36', 'SiPt38']\n",
      "5932/5932 [==============================] - 2s 280us/sample - loss: 0.4491 - acc: 0.7756\n",
      "5932/5932 [==============================] - 2s 270us/sample - loss: 0.6985 - acc: 0.6342\n",
      "Train on 59983 samples, validate on 5932 samples\n",
      "Epoch 1/1000\n",
      "59983/59983 [==============================] - 11s 178us/sample - loss: 0.9580 - acc: 0.6175 - val_loss: 0.5288 - val_acc: 0.7050\n",
      "Epoch 2/1000\n",
      "59983/59983 [==============================] - 4s 65us/sample - loss: 0.6626 - acc: 0.6754 - val_loss: 0.5320 - val_acc: 0.6952\n",
      "Epoch 3/1000\n",
      "59983/59983 [==============================] - 4s 67us/sample - loss: 0.5832 - acc: 0.7173 - val_loss: 0.5525 - val_acc: 0.7215\n",
      "Epoch 4/1000\n",
      "59983/59983 [==============================] - 4s 63us/sample - loss: 0.5475 - acc: 0.7429 - val_loss: 0.5458 - val_acc: 0.6962\n",
      "Epoch 5/1000\n",
      "59983/59983 [==============================] - 4s 63us/sample - loss: 0.5229 - acc: 0.7601 - val_loss: 0.5664 - val_acc: 0.7030\n",
      "Epoch 6/1000\n",
      "59983/59983 [==============================] - 4s 61us/sample - loss: 0.5023 - acc: 0.7772 - val_loss: 0.6395 - val_acc: 0.6891\n",
      "Epoch 7/1000\n",
      "59983/59983 [==============================] - 4s 64us/sample - loss: 0.4880 - acc: 0.7877 - val_loss: 0.4989 - val_acc: 0.7321\n",
      "Epoch 8/1000\n",
      "59983/59983 [==============================] - 4s 63us/sample - loss: 0.4688 - acc: 0.7986 - val_loss: 0.5367 - val_acc: 0.7254\n",
      "Epoch 9/1000\n",
      "59983/59983 [==============================] - 4s 63us/sample - loss: 0.4552 - acc: 0.8083 - val_loss: 0.5565 - val_acc: 0.7202\n",
      "Epoch 10/1000\n",
      "59983/59983 [==============================] - 4s 62us/sample - loss: 0.4396 - acc: 0.8172 - val_loss: 0.5750 - val_acc: 0.7185\n",
      "Epoch 11/1000\n",
      "59983/59983 [==============================] - 4s 66us/sample - loss: 0.4300 - acc: 0.8217 - val_loss: 0.6122 - val_acc: 0.7203\n",
      "Epoch 12/1000\n",
      "59983/59983 [==============================] - 4s 64us/sample - loss: 0.4194 - acc: 0.8264 - val_loss: 0.6313 - val_acc: 0.7100\n",
      "Epoch 13/1000\n",
      "59983/59983 [==============================] - 4s 63us/sample - loss: 0.4099 - acc: 0.8308 - val_loss: 0.6517 - val_acc: 0.7099\n",
      "Epoch 14/1000\n",
      "59983/59983 [==============================] - 4s 63us/sample - loss: 0.3981 - acc: 0.8373 - val_loss: 0.5144 - val_acc: 0.7348\n",
      "Epoch 15/1000\n",
      "59983/59983 [==============================] - 4s 63us/sample - loss: 0.3859 - acc: 0.8431 - val_loss: 0.6063 - val_acc: 0.7321\n",
      "Epoch 16/1000\n",
      "59983/59983 [==============================] - 4s 62us/sample - loss: 0.3684 - acc: 0.8498 - val_loss: 0.5607 - val_acc: 0.7399\n",
      "Epoch 17/1000\n",
      "59983/59983 [==============================] - 4s 61us/sample - loss: 0.3538 - acc: 0.8569 - val_loss: 0.6936 - val_acc: 0.7158\n",
      "Epoch 18/1000\n",
      "59983/59983 [==============================] - 4s 63us/sample - loss: 0.3386 - acc: 0.8637 - val_loss: 0.7080 - val_acc: 0.7210\n",
      "Epoch 19/1000\n",
      "59983/59983 [==============================] - 4s 62us/sample - loss: 0.3187 - acc: 0.8726 - val_loss: 0.5815 - val_acc: 0.7512\n",
      "Epoch 20/1000\n",
      "59983/59983 [==============================] - 4s 62us/sample - loss: 0.3069 - acc: 0.8771 - val_loss: 0.6645 - val_acc: 0.7239\n",
      "Epoch 21/1000\n",
      "59200/59983 [============================>.] - ETA: 0s - loss: 0.2876 - acc: 0.8860\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "59983/59983 [==============================] - 5s 79us/sample - loss: 0.2877 - acc: 0.8859 - val_loss: 0.6247 - val_acc: 0.7616\n",
      "Epoch 22/1000\n",
      "59983/59983 [==============================] - 4s 63us/sample - loss: 0.2542 - acc: 0.8998 - val_loss: 0.8324 - val_acc: 0.6585\n",
      "Epoch 23/1000\n",
      "59983/59983 [==============================] - 4s 64us/sample - loss: 0.2436 - acc: 0.9040 - val_loss: 0.6580 - val_acc: 0.7277\n",
      "Epoch 24/1000\n",
      "59983/59983 [==============================] - 4s 65us/sample - loss: 0.2371 - acc: 0.9088 - val_loss: 0.7775 - val_acc: 0.6490\n",
      "Epoch 25/1000\n",
      "59983/59983 [==============================] - 4s 68us/sample - loss: 0.2250 - acc: 0.9134 - val_loss: 0.7142 - val_acc: 0.7026\n",
      "Epoch 26/1000\n",
      "59983/59983 [==============================] - 4s 67us/sample - loss: 0.2180 - acc: 0.9156 - val_loss: 1.0348 - val_acc: 0.5831\n",
      "Epoch 27/1000\n",
      "58400/59983 [============================>.] - ETA: 0s - loss: 0.2113 - acc: 0.9180Restoring model weights from the end of the best epoch.\n",
      "59983/59983 [==============================] - 4s 69us/sample - loss: 0.2108 - acc: 0.9183 - val_loss: 0.8365 - val_acc: 0.6696\n",
      "Epoch 00027: early stopping\n",
      "0.8095238095238095 0.6551724137931034 0.7368421052631577\n",
      "['GaCo04', 'GaPt03', 'GaPt07', 'GaPt27', 'JuCo20', 'JuCo24', 'JuPt03', 'JuPt06', 'JuPt10', 'JuPt14', 'JuPt26', 'SiCo03', 'SiCo04', 'SiCo10', 'SiCo24', 'SiPt21', 'SiPt23']\n",
      "8097/8097 [==============================] - 3s 339us/sample - loss: 0.3495 - acc: 0.8578\n",
      "8097/8097 [==============================] - 3s 315us/sample - loss: 0.6393 - acc: 0.6879\n",
      "Train on 57818 samples, validate on 8097 samples\n",
      "Epoch 1/1000\n",
      "57818/57818 [==============================] - 13s 222us/sample - loss: 0.9818 - acc: 0.6078 - val_loss: 0.4879 - val_acc: 0.7884\n",
      "Epoch 2/1000\n",
      "57818/57818 [==============================] - 4s 69us/sample - loss: 0.6660 - acc: 0.6661 - val_loss: 0.4331 - val_acc: 0.7852\n",
      "Epoch 3/1000\n",
      "57818/57818 [==============================] - 4s 69us/sample - loss: 0.6029 - acc: 0.7017 - val_loss: 0.4230 - val_acc: 0.7967\n",
      "Epoch 4/1000\n",
      "57818/57818 [==============================] - 4s 63us/sample - loss: 0.5640 - acc: 0.7291 - val_loss: 0.4185 - val_acc: 0.8135\n",
      "Epoch 5/1000\n",
      "57818/57818 [==============================] - 4s 67us/sample - loss: 0.5388 - acc: 0.7463 - val_loss: 0.4462 - val_acc: 0.8254\n",
      "Epoch 6/1000\n",
      "57818/57818 [==============================] - 4s 68us/sample - loss: 0.5205 - acc: 0.7612 - val_loss: 0.4613 - val_acc: 0.7989\n",
      "Epoch 7/1000\n",
      "57818/57818 [==============================] - 4s 64us/sample - loss: 0.4989 - acc: 0.7771 - val_loss: 0.4253 - val_acc: 0.8019\n",
      "Epoch 8/1000\n",
      "57818/57818 [==============================] - 4s 65us/sample - loss: 0.4866 - acc: 0.7858 - val_loss: 0.3789 - val_acc: 0.8420\n",
      "Epoch 9/1000\n",
      "57818/57818 [==============================] - 4s 63us/sample - loss: 0.4733 - acc: 0.7952 - val_loss: 0.4525 - val_acc: 0.7978\n",
      "Epoch 10/1000\n",
      "57818/57818 [==============================] - 4s 63us/sample - loss: 0.4604 - acc: 0.8029 - val_loss: 0.4118 - val_acc: 0.8215\n",
      "Epoch 11/1000\n",
      "57818/57818 [==============================] - 4s 63us/sample - loss: 0.4483 - acc: 0.8097 - val_loss: 0.3810 - val_acc: 0.8403\n",
      "Epoch 12/1000\n",
      "57818/57818 [==============================] - 4s 64us/sample - loss: 0.4322 - acc: 0.8186 - val_loss: 0.4086 - val_acc: 0.8164\n",
      "Epoch 13/1000\n",
      "57818/57818 [==============================] - 4s 65us/sample - loss: 0.4200 - acc: 0.8253 - val_loss: 0.4878 - val_acc: 0.7749\n",
      "Epoch 14/1000\n",
      "57818/57818 [==============================] - 4s 63us/sample - loss: 0.4067 - acc: 0.8315 - val_loss: 0.4118 - val_acc: 0.8241\n",
      "Epoch 15/1000\n",
      "57818/57818 [==============================] - 4s 65us/sample - loss: 0.3864 - acc: 0.8431 - val_loss: 0.4789 - val_acc: 0.7913\n",
      "Epoch 16/1000\n",
      "57818/57818 [==============================] - 4s 64us/sample - loss: 0.3710 - acc: 0.8500 - val_loss: 0.4419 - val_acc: 0.8134\n",
      "Epoch 17/1000\n",
      "57818/57818 [==============================] - 4s 67us/sample - loss: 0.3544 - acc: 0.8567 - val_loss: 0.5464 - val_acc: 0.7456\n",
      "Epoch 18/1000\n",
      "57818/57818 [==============================] - 4s 66us/sample - loss: 0.3332 - acc: 0.8680 - val_loss: 0.5464 - val_acc: 0.7637\n",
      "Epoch 19/1000\n",
      "57818/57818 [==============================] - 4s 63us/sample - loss: 0.3215 - acc: 0.8742 - val_loss: 0.4898 - val_acc: 0.7918\n",
      "Epoch 20/1000\n",
      "57818/57818 [==============================] - 4s 63us/sample - loss: 0.3048 - acc: 0.8802 - val_loss: 0.4952 - val_acc: 0.7850\n",
      "Epoch 21/1000\n",
      "57818/57818 [==============================] - 4s 67us/sample - loss: 0.2887 - acc: 0.8874 - val_loss: 0.5615 - val_acc: 0.7705\n",
      "Epoch 22/1000\n",
      "57600/57818 [============================>.] - ETA: 0s - loss: 0.2761 - acc: 0.8930\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "57818/57818 [==============================] - 5s 91us/sample - loss: 0.2760 - acc: 0.8930 - val_loss: 0.6095 - val_acc: 0.7634\n",
      "Epoch 23/1000\n",
      "57818/57818 [==============================] - 4s 64us/sample - loss: 0.2399 - acc: 0.9087 - val_loss: 0.6311 - val_acc: 0.7175\n",
      "Epoch 24/1000\n",
      "57818/57818 [==============================] - 4s 62us/sample - loss: 0.2300 - acc: 0.9128 - val_loss: 0.5220 - val_acc: 0.7707\n",
      "Epoch 25/1000\n",
      "57818/57818 [==============================] - 4s 64us/sample - loss: 0.2227 - acc: 0.9149 - val_loss: 0.7289 - val_acc: 0.7579\n",
      "Epoch 26/1000\n",
      "57818/57818 [==============================] - 4s 66us/sample - loss: 0.2140 - acc: 0.9183 - val_loss: 0.5701 - val_acc: 0.7818\n",
      "Epoch 27/1000\n",
      "57818/57818 [==============================] - 4s 66us/sample - loss: 0.2065 - acc: 0.9204 - val_loss: 0.7082 - val_acc: 0.7514\n",
      "Epoch 28/1000\n",
      "56800/57818 [============================>.] - ETA: 0s - loss: 0.2028 - acc: 0.9237Restoring model weights from the end of the best epoch.\n",
      "57818/57818 [==============================] - 4s 64us/sample - loss: 0.2024 - acc: 0.9238 - val_loss: 0.5924 - val_acc: 0.7740\n",
      "Epoch 00028: early stopping\n",
      "0.9236111111111112 0.8863636363636364 0.9350649350649352\n",
      "['GaCo01', 'GaCo02', 'GaCo12', 'GaPt21', 'GaPt22', 'GaPt24', 'GaPt29', 'GaPt30', 'JuCo21', 'JuPt16', 'SiCo08', 'SiCo20', 'SiCo22', 'SiPt07', 'SiPt09', 'SiPt10', 'SiPt15']\n",
      "6739/6739 [==============================] - 2s 313us/sample - loss: 0.4100 - acc: 0.8320\n",
      "6739/6739 [==============================] - 2s 302us/sample - loss: 0.7577 - acc: 0.5689\n",
      "Train on 59176 samples, validate on 6739 samples\n",
      "Epoch 1/1000\n",
      "59176/59176 [==============================] - 14s 241us/sample - loss: 0.9834 - acc: 0.6155 - val_loss: 0.5582 - val_acc: 0.7305\n",
      "Epoch 2/1000\n",
      "59176/59176 [==============================] - 4s 75us/sample - loss: 0.6559 - acc: 0.6739 - val_loss: 0.5562 - val_acc: 0.7344\n",
      "Epoch 3/1000\n",
      "59176/59176 [==============================] - 4s 71us/sample - loss: 0.5892 - acc: 0.7100 - val_loss: 0.5199 - val_acc: 0.7871\n",
      "Epoch 4/1000\n",
      "59176/59176 [==============================] - 4s 70us/sample - loss: 0.5567 - acc: 0.7317 - val_loss: 0.5118 - val_acc: 0.8216\n",
      "Epoch 5/1000\n",
      "59176/59176 [==============================] - 4s 72us/sample - loss: 0.5265 - acc: 0.7534 - val_loss: 0.5086 - val_acc: 0.8215\n",
      "Epoch 6/1000\n",
      "59176/59176 [==============================] - 4s 63us/sample - loss: 0.5071 - acc: 0.7676 - val_loss: 0.4964 - val_acc: 0.7452\n",
      "Epoch 7/1000\n",
      "59176/59176 [==============================] - 4s 63us/sample - loss: 0.4887 - acc: 0.7808 - val_loss: 0.5422 - val_acc: 0.7044\n",
      "Epoch 8/1000\n",
      "59176/59176 [==============================] - 4s 63us/sample - loss: 0.4775 - acc: 0.7874 - val_loss: 0.5638 - val_acc: 0.6821\n",
      "Epoch 9/1000\n",
      "59176/59176 [==============================] - 4s 63us/sample - loss: 0.4639 - acc: 0.7972 - val_loss: 0.5579 - val_acc: 0.6949\n",
      "Epoch 10/1000\n",
      "59176/59176 [==============================] - 4s 67us/sample - loss: 0.4537 - acc: 0.8050 - val_loss: 0.4813 - val_acc: 0.7933\n",
      "Epoch 11/1000\n",
      "59176/59176 [==============================] - 4s 63us/sample - loss: 0.4369 - acc: 0.8164 - val_loss: 0.5193 - val_acc: 0.7688\n",
      "Epoch 12/1000\n",
      "59176/59176 [==============================] - 4s 63us/sample - loss: 0.4247 - acc: 0.8226 - val_loss: 0.5373 - val_acc: 0.7311\n",
      "Epoch 13/1000\n",
      "59176/59176 [==============================] - 4s 61us/sample - loss: 0.4126 - acc: 0.8266 - val_loss: 0.5292 - val_acc: 0.7399\n",
      "Epoch 14/1000\n",
      "59176/59176 [==============================] - 4s 63us/sample - loss: 0.3994 - acc: 0.8335 - val_loss: 0.5111 - val_acc: 0.8025\n",
      "Epoch 15/1000\n",
      "59176/59176 [==============================] - 4s 62us/sample - loss: 0.3821 - acc: 0.8414 - val_loss: 0.5404 - val_acc: 0.7083\n",
      "Epoch 16/1000\n",
      "59176/59176 [==============================] - 4s 62us/sample - loss: 0.3707 - acc: 0.8465 - val_loss: 0.4767 - val_acc: 0.7747\n",
      "Epoch 17/1000\n",
      "59176/59176 [==============================] - 4s 63us/sample - loss: 0.3537 - acc: 0.8568 - val_loss: 0.4790 - val_acc: 0.7881\n",
      "Epoch 18/1000\n",
      "59176/59176 [==============================] - 4s 64us/sample - loss: 0.3419 - acc: 0.8605 - val_loss: 0.5025 - val_acc: 0.7782\n",
      "Epoch 19/1000\n",
      "59176/59176 [==============================] - 4s 66us/sample - loss: 0.3316 - acc: 0.8655 - val_loss: 0.5263 - val_acc: 0.7446\n",
      "Epoch 20/1000\n",
      "59176/59176 [==============================] - 4s 65us/sample - loss: 0.3184 - acc: 0.8704 - val_loss: 0.5107 - val_acc: 0.7721\n",
      "Epoch 21/1000\n",
      "59176/59176 [==============================] - 4s 61us/sample - loss: 0.2978 - acc: 0.8813 - val_loss: 0.5324 - val_acc: 0.7696\n",
      "Epoch 22/1000\n",
      "59176/59176 [==============================] - 4s 61us/sample - loss: 0.2852 - acc: 0.8864 - val_loss: 0.5135 - val_acc: 0.7884\n",
      "Epoch 23/1000\n",
      "59176/59176 [==============================] - 4s 61us/sample - loss: 0.2668 - acc: 0.8944 - val_loss: 0.5372 - val_acc: 0.7657\n",
      "Epoch 24/1000\n",
      "59176/59176 [==============================] - 4s 61us/sample - loss: 0.2555 - acc: 0.8990 - val_loss: 0.6746 - val_acc: 0.6951\n",
      "Epoch 25/1000\n",
      "59176/59176 [==============================] - 4s 63us/sample - loss: 0.2419 - acc: 0.9053 - val_loss: 0.5402 - val_acc: 0.7834\n",
      "Epoch 26/1000\n",
      "59176/59176 [==============================] - 4s 65us/sample - loss: 0.2326 - acc: 0.9084 - val_loss: 0.4562 - val_acc: 0.8296\n",
      "Epoch 27/1000\n",
      "59176/59176 [==============================] - 4s 65us/sample - loss: 0.2216 - acc: 0.9139 - val_loss: 0.5170 - val_acc: 0.8000\n",
      "Epoch 28/1000\n",
      "59176/59176 [==============================] - 4s 62us/sample - loss: 0.2116 - acc: 0.9173 - val_loss: 0.5640 - val_acc: 0.7854\n",
      "Epoch 29/1000\n",
      "59176/59176 [==============================] - 4s 63us/sample - loss: 0.1981 - acc: 0.9238 - val_loss: 0.5692 - val_acc: 0.7844\n",
      "Epoch 30/1000\n",
      "59176/59176 [==============================] - 4s 63us/sample - loss: 0.1907 - acc: 0.9273 - val_loss: 0.5739 - val_acc: 0.7817\n",
      "Epoch 31/1000\n",
      "59176/59176 [==============================] - 4s 63us/sample - loss: 0.1866 - acc: 0.9283 - val_loss: 0.5215 - val_acc: 0.8077\n",
      "Epoch 32/1000\n",
      "59176/59176 [==============================] - 4s 62us/sample - loss: 0.1762 - acc: 0.9331 - val_loss: 0.5968 - val_acc: 0.7779\n",
      "Epoch 33/1000\n",
      "59176/59176 [==============================] - 4s 62us/sample - loss: 0.1713 - acc: 0.9352 - val_loss: 0.5843 - val_acc: 0.7977\n",
      "Epoch 34/1000\n",
      "59176/59176 [==============================] - 4s 65us/sample - loss: 0.1695 - acc: 0.9365 - val_loss: 0.5189 - val_acc: 0.7942\n",
      "Epoch 35/1000\n",
      "59176/59176 [==============================] - 4s 64us/sample - loss: 0.1616 - acc: 0.9407 - val_loss: 0.4957 - val_acc: 0.8052\n",
      "Epoch 36/1000\n",
      "59176/59176 [==============================] - 4s 69us/sample - loss: 0.1544 - acc: 0.9433 - val_loss: 0.5987 - val_acc: 0.7636\n",
      "Epoch 37/1000\n",
      "59176/59176 [==============================] - 4s 70us/sample - loss: 0.1485 - acc: 0.9459 - val_loss: 0.6318 - val_acc: 0.7764\n",
      "Epoch 38/1000\n",
      "59176/59176 [==============================] - 4s 70us/sample - loss: 0.1436 - acc: 0.9474 - val_loss: 0.6094 - val_acc: 0.7638\n",
      "Epoch 39/1000\n",
      "59176/59176 [==============================] - 4s 71us/sample - loss: 0.1421 - acc: 0.9493 - val_loss: 0.6260 - val_acc: 0.7737\n",
      "Epoch 40/1000\n",
      "57600/59176 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.9502\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "59176/59176 [==============================] - 5s 90us/sample - loss: 0.1393 - acc: 0.9500 - val_loss: 0.5391 - val_acc: 0.7825\n",
      "Epoch 41/1000\n",
      "59176/59176 [==============================] - 4s 72us/sample - loss: 0.1192 - acc: 0.9583 - val_loss: 0.6645 - val_acc: 0.7575\n",
      "Epoch 42/1000\n",
      "59176/59176 [==============================] - 4s 69us/sample - loss: 0.1169 - acc: 0.9579 - val_loss: 0.6695 - val_acc: 0.7345\n",
      "Epoch 43/1000\n",
      "59176/59176 [==============================] - 4s 67us/sample - loss: 0.1126 - acc: 0.9598 - val_loss: 0.6325 - val_acc: 0.7443\n",
      "Epoch 44/1000\n",
      "59176/59176 [==============================] - 4s 68us/sample - loss: 0.1114 - acc: 0.9607 - val_loss: 0.6587 - val_acc: 0.7354\n",
      "Epoch 45/1000\n",
      "59176/59176 [==============================] - 4s 67us/sample - loss: 0.1080 - acc: 0.9613 - val_loss: 0.5561 - val_acc: 0.7885\n",
      "Epoch 46/1000\n",
      "57600/59176 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9615Restoring model weights from the end of the best epoch.\n",
      "59176/59176 [==============================] - 4s 63us/sample - loss: 0.1097 - acc: 0.9614 - val_loss: 0.6970 - val_acc: 0.7365\n",
      "Epoch 00046: early stopping\n",
      "0.9312499999999999 0.8928571428571429 0.9268292682926829\n",
      "['GaCo05', 'GaCo09', 'GaCo13', 'GaCo15', 'GaPt09', 'GaPt19', 'GaPt20', 'GaPt33', 'JuCo17', 'JuCo23', 'JuPt13', 'JuPt21', 'JuPt27', 'SiCo23', 'SiPt16', 'SiPt29', 'SiPt35']\n",
      "9146/9146 [==============================] - 3s 285us/sample - loss: 0.1086 - acc: 0.9614\n",
      "9146/9146 [==============================] - 3s 273us/sample - loss: 0.7021 - acc: 0.6748\n",
      "Train on 56769 samples, validate on 9146 samples\n",
      "Epoch 1/1000\n",
      "56769/56769 [==============================] - 14s 240us/sample - loss: 0.9517 - acc: 0.6267 - val_loss: 0.6533 - val_acc: 0.6777\n",
      "Epoch 2/1000\n",
      "56769/56769 [==============================] - 4s 70us/sample - loss: 0.6537 - acc: 0.6780 - val_loss: 0.6738 - val_acc: 0.5880\n",
      "Epoch 3/1000\n",
      "56769/56769 [==============================] - 4s 67us/sample - loss: 0.5956 - acc: 0.7062 - val_loss: 0.6358 - val_acc: 0.7009\n",
      "Epoch 4/1000\n",
      "56769/56769 [==============================] - 4s 66us/sample - loss: 0.5614 - acc: 0.7326 - val_loss: 0.5861 - val_acc: 0.7226\n",
      "Epoch 5/1000\n",
      "56769/56769 [==============================] - 4s 66us/sample - loss: 0.5341 - acc: 0.7524 - val_loss: 0.6008 - val_acc: 0.7098\n",
      "Epoch 6/1000\n",
      "56769/56769 [==============================] - 4s 65us/sample - loss: 0.5112 - acc: 0.7684 - val_loss: 0.5840 - val_acc: 0.7233\n",
      "Epoch 7/1000\n",
      "56769/56769 [==============================] - 4s 66us/sample - loss: 0.4931 - acc: 0.7819 - val_loss: 0.5829 - val_acc: 0.7093\n",
      "Epoch 8/1000\n",
      "56769/56769 [==============================] - 4s 65us/sample - loss: 0.4808 - acc: 0.7904 - val_loss: 0.5933 - val_acc: 0.7136\n",
      "Epoch 9/1000\n",
      "56769/56769 [==============================] - 4s 66us/sample - loss: 0.4583 - acc: 0.8038 - val_loss: 0.6925 - val_acc: 0.6366\n",
      "Epoch 10/1000\n",
      "56769/56769 [==============================] - 4s 65us/sample - loss: 0.4470 - acc: 0.8127 - val_loss: 0.6061 - val_acc: 0.6937\n",
      "Epoch 11/1000\n",
      "56769/56769 [==============================] - 4s 66us/sample - loss: 0.4307 - acc: 0.8202 - val_loss: 0.7432 - val_acc: 0.6228\n",
      "Epoch 12/1000\n",
      "56769/56769 [==============================] - 4s 65us/sample - loss: 0.4160 - acc: 0.8287 - val_loss: 0.6193 - val_acc: 0.6888\n",
      "Epoch 13/1000\n",
      "56769/56769 [==============================] - 4s 63us/sample - loss: 0.4085 - acc: 0.8321 - val_loss: 0.7443 - val_acc: 0.6299\n",
      "Epoch 14/1000\n",
      "56769/56769 [==============================] - 4s 63us/sample - loss: 0.3883 - acc: 0.8412 - val_loss: 0.7301 - val_acc: 0.6630\n",
      "Epoch 15/1000\n",
      "56769/56769 [==============================] - 4s 67us/sample - loss: 0.3692 - acc: 0.8509 - val_loss: 0.8583 - val_acc: 0.6401\n",
      "Epoch 16/1000\n",
      "56769/56769 [==============================] - 4s 67us/sample - loss: 0.3523 - acc: 0.8581 - val_loss: 0.8237 - val_acc: 0.6285\n",
      "Epoch 17/1000\n",
      "56769/56769 [==============================] - 4s 67us/sample - loss: 0.3339 - acc: 0.8655 - val_loss: 0.8624 - val_acc: 0.6309\n",
      "Epoch 18/1000\n",
      "56769/56769 [==============================] - 4s 66us/sample - loss: 0.3211 - acc: 0.8730 - val_loss: 0.9218 - val_acc: 0.5763\n",
      "Epoch 19/1000\n",
      "56769/56769 [==============================] - 4s 63us/sample - loss: 0.3024 - acc: 0.8799 - val_loss: 1.0548 - val_acc: 0.6498\n",
      "Epoch 20/1000\n",
      "56769/56769 [==============================] - 4s 63us/sample - loss: 0.2876 - acc: 0.8861 - val_loss: 1.1325 - val_acc: 0.5694\n",
      "Epoch 21/1000\n",
      "55200/56769 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.8944\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "56769/56769 [==============================] - 6s 98us/sample - loss: 0.2725 - acc: 0.8946 - val_loss: 1.2066 - val_acc: 0.5631\n",
      "Epoch 22/1000\n",
      "56769/56769 [==============================] - 4s 65us/sample - loss: 0.2381 - acc: 0.9063 - val_loss: 1.1096 - val_acc: 0.6467\n",
      "Epoch 23/1000\n",
      "56769/56769 [==============================] - 4s 67us/sample - loss: 0.2302 - acc: 0.9096 - val_loss: 1.3559 - val_acc: 0.6115\n",
      "Epoch 24/1000\n",
      "56000/56769 [============================>.] - ETA: 0s - loss: 0.2223 - acc: 0.9121Restoring model weights from the end of the best epoch.\n",
      "56769/56769 [==============================] - 4s 69us/sample - loss: 0.2226 - acc: 0.9120 - val_loss: 1.3517 - val_acc: 0.6184\n",
      "Epoch 00024: early stopping\n",
      "0.7491638795986622 0.7777777777777778 0.8518518518518519\n",
      "['GaCo14', 'GaPt13', 'GaPt14', 'GaPt18', 'JuCo01', 'JuCo06', 'JuCo13', 'JuPt07', 'SiCo09', 'SiCo14', 'SiCo15', 'SiPt12', 'SiPt18', 'SiPt19', 'SiPt25', 'SiPt37']\n",
      "5423/5423 [==============================] - 2s 312us/sample - loss: 0.5378 - acc: 0.7455\n",
      "5423/5423 [==============================] - 2s 296us/sample - loss: 0.8106 - acc: 0.5405\n",
      "Train on 60492 samples, validate on 5423 samples\n",
      "Epoch 1/1000\n",
      "60492/60492 [==============================] - 14s 228us/sample - loss: 0.9811 - acc: 0.6277 - val_loss: 0.6380 - val_acc: 0.6452\n",
      "Epoch 2/1000\n",
      "60492/60492 [==============================] - 4s 64us/sample - loss: 0.6462 - acc: 0.6872 - val_loss: 1.0763 - val_acc: 0.6388\n",
      "Epoch 3/1000\n",
      "60492/60492 [==============================] - 4s 64us/sample - loss: 0.5813 - acc: 0.7175 - val_loss: 0.6093 - val_acc: 0.6852\n",
      "Epoch 4/1000\n",
      "60492/60492 [==============================] - 4s 65us/sample - loss: 0.5430 - acc: 0.7460 - val_loss: 0.6509 - val_acc: 0.6360\n",
      "Epoch 5/1000\n",
      "60492/60492 [==============================] - 4s 64us/sample - loss: 0.5159 - acc: 0.7655 - val_loss: 0.6341 - val_acc: 0.6622\n",
      "Epoch 6/1000\n",
      "60492/60492 [==============================] - 4s 64us/sample - loss: 0.4923 - acc: 0.7819 - val_loss: 0.6565 - val_acc: 0.6537\n",
      "Epoch 7/1000\n",
      "60492/60492 [==============================] - 4s 64us/sample - loss: 0.4753 - acc: 0.7926 - val_loss: 0.6343 - val_acc: 0.6812\n",
      "Epoch 8/1000\n",
      "60492/60492 [==============================] - 4s 65us/sample - loss: 0.4639 - acc: 0.7990 - val_loss: 0.6311 - val_acc: 0.6965\n",
      "Epoch 9/1000\n",
      "60492/60492 [==============================] - 4s 65us/sample - loss: 0.4534 - acc: 0.8061 - val_loss: 0.6490 - val_acc: 0.6734\n",
      "Epoch 10/1000\n",
      "60492/60492 [==============================] - 4s 64us/sample - loss: 0.4427 - acc: 0.8129 - val_loss: 0.5910 - val_acc: 0.7107\n",
      "Epoch 11/1000\n",
      "60492/60492 [==============================] - 4s 64us/sample - loss: 0.4309 - acc: 0.8185 - val_loss: 0.6117 - val_acc: 0.7173\n",
      "Epoch 12/1000\n",
      "60492/60492 [==============================] - 4s 66us/sample - loss: 0.4211 - acc: 0.8225 - val_loss: 0.6133 - val_acc: 0.7133\n",
      "Epoch 13/1000\n",
      "60492/60492 [==============================] - 4s 64us/sample - loss: 0.4131 - acc: 0.8276 - val_loss: 0.6511 - val_acc: 0.6911\n",
      "Epoch 14/1000\n",
      "60492/60492 [==============================] - 4s 63us/sample - loss: 0.4025 - acc: 0.8320 - val_loss: 0.5778 - val_acc: 0.7195\n",
      "Epoch 15/1000\n",
      "60492/60492 [==============================] - 4s 63us/sample - loss: 0.3891 - acc: 0.8391 - val_loss: 0.6572 - val_acc: 0.7033\n",
      "Epoch 16/1000\n",
      "60492/60492 [==============================] - 4s 63us/sample - loss: 0.3797 - acc: 0.8439 - val_loss: 0.5747 - val_acc: 0.7358\n",
      "Epoch 17/1000\n",
      "60492/60492 [==============================] - 4s 63us/sample - loss: 0.3640 - acc: 0.8508 - val_loss: 0.6484 - val_acc: 0.7118\n",
      "Epoch 18/1000\n",
      "60492/60492 [==============================] - 4s 64us/sample - loss: 0.3484 - acc: 0.8608 - val_loss: 0.7142 - val_acc: 0.6998\n",
      "Epoch 19/1000\n",
      "60492/60492 [==============================] - 4s 63us/sample - loss: 0.3360 - acc: 0.8660 - val_loss: 0.5552 - val_acc: 0.7243\n",
      "Epoch 20/1000\n",
      "60492/60492 [==============================] - 4s 65us/sample - loss: 0.3242 - acc: 0.8738 - val_loss: 0.6062 - val_acc: 0.7157\n",
      "Epoch 21/1000\n",
      "60492/60492 [==============================] - 4s 65us/sample - loss: 0.3082 - acc: 0.8786 - val_loss: 0.6609 - val_acc: 0.7147\n",
      "Epoch 22/1000\n",
      "60492/60492 [==============================] - 4s 65us/sample - loss: 0.2933 - acc: 0.8845 - val_loss: 0.7643 - val_acc: 0.7169\n",
      "Epoch 23/1000\n",
      "60492/60492 [==============================] - 4s 66us/sample - loss: 0.2776 - acc: 0.8906 - val_loss: 0.4881 - val_acc: 0.7524\n",
      "Epoch 24/1000\n",
      "60492/60492 [==============================] - 4s 64us/sample - loss: 0.2695 - acc: 0.8950 - val_loss: 0.5768 - val_acc: 0.7599\n",
      "Epoch 25/1000\n",
      "60492/60492 [==============================] - 4s 64us/sample - loss: 0.2543 - acc: 0.9008 - val_loss: 0.7342 - val_acc: 0.7293\n",
      "Epoch 26/1000\n",
      "60492/60492 [==============================] - 4s 65us/sample - loss: 0.2452 - acc: 0.9051 - val_loss: 0.6470 - val_acc: 0.7254\n",
      "Epoch 27/1000\n",
      "60492/60492 [==============================] - 4s 63us/sample - loss: 0.2346 - acc: 0.9090 - val_loss: 0.6906 - val_acc: 0.7583\n",
      "Epoch 28/1000\n",
      "60492/60492 [==============================] - 4s 63us/sample - loss: 0.2286 - acc: 0.9127 - val_loss: 0.6233 - val_acc: 0.7042\n",
      "Epoch 29/1000\n",
      "60492/60492 [==============================] - 4s 64us/sample - loss: 0.2128 - acc: 0.9192 - val_loss: 0.6365 - val_acc: 0.7280\n",
      "Epoch 30/1000\n",
      "60492/60492 [==============================] - 4s 63us/sample - loss: 0.2059 - acc: 0.9205 - val_loss: 0.6470 - val_acc: 0.7477\n",
      "Epoch 31/1000\n",
      "60492/60492 [==============================] - 4s 64us/sample - loss: 0.1966 - acc: 0.9249 - val_loss: 0.6287 - val_acc: 0.7470\n",
      "Epoch 32/1000\n",
      "60492/60492 [==============================] - 4s 64us/sample - loss: 0.1912 - acc: 0.9278 - val_loss: 0.7181 - val_acc: 0.7297\n",
      "Epoch 33/1000\n",
      "60492/60492 [==============================] - 4s 64us/sample - loss: 0.1831 - acc: 0.9310 - val_loss: 0.6313 - val_acc: 0.7536\n",
      "Epoch 34/1000\n",
      "60492/60492 [==============================] - 4s 64us/sample - loss: 0.1788 - acc: 0.9326 - val_loss: 0.7232 - val_acc: 0.7256\n",
      "Epoch 35/1000\n",
      "60492/60492 [==============================] - 4s 67us/sample - loss: 0.1720 - acc: 0.9350 - val_loss: 0.5487 - val_acc: 0.7868\n",
      "Epoch 36/1000\n",
      "60492/60492 [==============================] - 4s 68us/sample - loss: 0.1674 - acc: 0.9367 - val_loss: 0.5974 - val_acc: 0.7472\n",
      "Epoch 37/1000\n",
      "60000/60492 [============================>.] - ETA: 0s - loss: 0.1621 - acc: 0.9391\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "60492/60492 [==============================] - 7s 113us/sample - loss: 0.1619 - acc: 0.9391 - val_loss: 0.6464 - val_acc: 0.7612\n",
      "Epoch 38/1000\n",
      "60492/60492 [==============================] - 4s 70us/sample - loss: 0.1398 - acc: 0.9491 - val_loss: 0.6262 - val_acc: 0.7796\n",
      "Epoch 39/1000\n",
      "60492/60492 [==============================] - 4s 69us/sample - loss: 0.1357 - acc: 0.9502 - val_loss: 0.6371 - val_acc: 0.7732\n",
      "Epoch 40/1000\n",
      "60492/60492 [==============================] - 4s 66us/sample - loss: 0.1354 - acc: 0.9505 - val_loss: 0.5175 - val_acc: 0.7997\n",
      "Epoch 41/1000\n",
      "60492/60492 [==============================] - 4s 64us/sample - loss: 0.1305 - acc: 0.9522 - val_loss: 0.6757 - val_acc: 0.7710\n",
      "Epoch 42/1000\n",
      "60492/60492 [==============================] - 4s 63us/sample - loss: 0.1316 - acc: 0.9520 - val_loss: 0.6228 - val_acc: 0.7846\n",
      "Epoch 43/1000\n",
      "60000/60492 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9532Restoring model weights from the end of the best epoch.\n",
      "60492/60492 [==============================] - 4s 64us/sample - loss: 0.1290 - acc: 0.9532 - val_loss: 0.6282 - val_acc: 0.7634\n",
      "Epoch 00043: early stopping\n",
      "0.9037037037037037 0.7083333333333334 0.7741935483870969\n",
      "['GaCo03', 'GaPt25', 'GaPt32', 'JuCo04', 'JuCo11', 'JuCo14', 'JuPt15', 'JuPt18', 'JuPt22', 'JuPt28', 'SiCo12', 'SiCo29', 'SiCo30', 'SiPt04', 'SiPt14', 'SiPt22']\n",
      "7065/7065 [==============================] - 3s 374us/sample - loss: 0.1852 - acc: 0.9270\n",
      "7065/7065 [==============================] - 3s 372us/sample - loss: 0.6726 - acc: 0.6515\n",
      "Train on 58850 samples, validate on 7065 samples\n",
      "Epoch 1/1000\n",
      "58850/58850 [==============================] - 14s 245us/sample - loss: 0.9493 - acc: 0.6210 - val_loss: 0.8679 - val_acc: 0.5439\n",
      "Epoch 2/1000\n",
      "58850/58850 [==============================] - 4s 66us/sample - loss: 0.6423 - acc: 0.6822 - val_loss: 0.8056 - val_acc: 0.5909\n",
      "Epoch 3/1000\n",
      "58850/58850 [==============================] - 4s 65us/sample - loss: 0.5845 - acc: 0.7161 - val_loss: 0.8877 - val_acc: 0.5713\n",
      "Epoch 4/1000\n",
      "58850/58850 [==============================] - 4s 66us/sample - loss: 0.5463 - acc: 0.7413 - val_loss: 0.7816 - val_acc: 0.6041\n",
      "Epoch 5/1000\n",
      "58850/58850 [==============================] - 4s 65us/sample - loss: 0.5252 - acc: 0.7600 - val_loss: 0.6277 - val_acc: 0.6293\n",
      "Epoch 6/1000\n",
      "58850/58850 [==============================] - 4s 66us/sample - loss: 0.5061 - acc: 0.7704 - val_loss: 0.5759 - val_acc: 0.6800\n",
      "Epoch 7/1000\n",
      "58850/58850 [==============================] - 4s 66us/sample - loss: 0.4906 - acc: 0.7826 - val_loss: 0.7484 - val_acc: 0.6395\n",
      "Epoch 8/1000\n",
      "58850/58850 [==============================] - 4s 67us/sample - loss: 0.4790 - acc: 0.7886 - val_loss: 0.5675 - val_acc: 0.6954\n",
      "Epoch 9/1000\n",
      "58850/58850 [==============================] - 4s 68us/sample - loss: 0.4669 - acc: 0.7970 - val_loss: 0.6550 - val_acc: 0.6436\n",
      "Epoch 10/1000\n",
      "58850/58850 [==============================] - 4s 68us/sample - loss: 0.4566 - acc: 0.8028 - val_loss: 0.7893 - val_acc: 0.6379\n",
      "Epoch 11/1000\n",
      "58850/58850 [==============================] - 4s 68us/sample - loss: 0.4425 - acc: 0.8087 - val_loss: 0.5819 - val_acc: 0.6692\n",
      "Epoch 12/1000\n",
      "58850/58850 [==============================] - 4s 68us/sample - loss: 0.4294 - acc: 0.8174 - val_loss: 0.5510 - val_acc: 0.7111\n",
      "Epoch 13/1000\n",
      "58850/58850 [==============================] - 4s 67us/sample - loss: 0.4161 - acc: 0.8242 - val_loss: 0.5017 - val_acc: 0.7703\n",
      "Epoch 14/1000\n",
      "58850/58850 [==============================] - 4s 67us/sample - loss: 0.4039 - acc: 0.8305 - val_loss: 0.6385 - val_acc: 0.7086\n",
      "Epoch 15/1000\n",
      "58850/58850 [==============================] - 4s 66us/sample - loss: 0.3869 - acc: 0.8401 - val_loss: 0.5860 - val_acc: 0.7045\n",
      "Epoch 16/1000\n",
      "58850/58850 [==============================] - 4s 66us/sample - loss: 0.3731 - acc: 0.8460 - val_loss: 0.5279 - val_acc: 0.7475\n",
      "Epoch 17/1000\n",
      "58850/58850 [==============================] - 4s 68us/sample - loss: 0.3597 - acc: 0.8538 - val_loss: 0.9805 - val_acc: 0.5883\n",
      "Epoch 18/1000\n",
      "58850/58850 [==============================] - 4s 67us/sample - loss: 0.3460 - acc: 0.8591 - val_loss: 0.5232 - val_acc: 0.7629\n",
      "Epoch 19/1000\n",
      "58850/58850 [==============================] - 4s 66us/sample - loss: 0.3244 - acc: 0.8683 - val_loss: 0.5125 - val_acc: 0.7410\n",
      "Epoch 20/1000\n",
      "58850/58850 [==============================] - 4s 67us/sample - loss: 0.3084 - acc: 0.8751 - val_loss: 0.5498 - val_acc: 0.6999\n",
      "Epoch 21/1000\n",
      "58850/58850 [==============================] - 4s 67us/sample - loss: 0.2921 - acc: 0.8813 - val_loss: 0.4086 - val_acc: 0.8072\n",
      "Epoch 22/1000\n",
      "58850/58850 [==============================] - 4s 68us/sample - loss: 0.2755 - acc: 0.8909 - val_loss: 0.4705 - val_acc: 0.7857\n",
      "Epoch 23/1000\n",
      "58850/58850 [==============================] - 4s 68us/sample - loss: 0.2624 - acc: 0.8958 - val_loss: 0.3109 - val_acc: 0.8786\n",
      "Epoch 24/1000\n",
      "58850/58850 [==============================] - 4s 69us/sample - loss: 0.2496 - acc: 0.9017 - val_loss: 0.5155 - val_acc: 0.7319\n",
      "Epoch 25/1000\n",
      "58850/58850 [==============================] - 4s 69us/sample - loss: 0.2424 - acc: 0.9051 - val_loss: 0.3571 - val_acc: 0.8490\n",
      "Epoch 26/1000\n",
      "58850/58850 [==============================] - 4s 69us/sample - loss: 0.2309 - acc: 0.9103 - val_loss: 0.5054 - val_acc: 0.7503\n",
      "Epoch 27/1000\n",
      "58850/58850 [==============================] - 4s 72us/sample - loss: 0.2174 - acc: 0.9161 - val_loss: 0.3774 - val_acc: 0.8389\n",
      "Epoch 28/1000\n",
      "58850/58850 [==============================] - 4s 70us/sample - loss: 0.2112 - acc: 0.9179 - val_loss: 0.3492 - val_acc: 0.8548\n",
      "Epoch 29/1000\n",
      "58850/58850 [==============================] - 4s 70us/sample - loss: 0.2021 - acc: 0.9225 - val_loss: 0.3896 - val_acc: 0.8527\n",
      "Epoch 30/1000\n",
      "58850/58850 [==============================] - 4s 69us/sample - loss: 0.1938 - acc: 0.9261 - val_loss: 0.3330 - val_acc: 0.8818\n",
      "Epoch 31/1000\n",
      "58850/58850 [==============================] - 4s 70us/sample - loss: 0.1870 - acc: 0.9306 - val_loss: 0.4808 - val_acc: 0.8408\n",
      "Epoch 32/1000\n",
      "58850/58850 [==============================] - 4s 71us/sample - loss: 0.1782 - acc: 0.9337 - val_loss: 0.4510 - val_acc: 0.8160\n",
      "Epoch 33/1000\n",
      "58850/58850 [==============================] - 4s 70us/sample - loss: 0.1722 - acc: 0.9358 - val_loss: 0.5524 - val_acc: 0.7996\n",
      "Epoch 34/1000\n",
      "58850/58850 [==============================] - 4s 70us/sample - loss: 0.1659 - acc: 0.9394 - val_loss: 0.2876 - val_acc: 0.8916\n",
      "Epoch 35/1000\n",
      "58850/58850 [==============================] - 4s 70us/sample - loss: 0.1639 - acc: 0.9408 - val_loss: 0.4118 - val_acc: 0.8317\n",
      "Epoch 36/1000\n",
      "58850/58850 [==============================] - 4s 69us/sample - loss: 0.1585 - acc: 0.9418 - val_loss: 0.5333 - val_acc: 0.7805\n",
      "Epoch 37/1000\n",
      "58850/58850 [==============================] - 4s 70us/sample - loss: 0.1523 - acc: 0.9446 - val_loss: 0.4190 - val_acc: 0.8613\n",
      "Epoch 38/1000\n",
      "58850/58850 [==============================] - 4s 74us/sample - loss: 0.1486 - acc: 0.9459 - val_loss: 0.4196 - val_acc: 0.8392\n",
      "Epoch 39/1000\n",
      "58850/58850 [==============================] - 4s 67us/sample - loss: 0.1477 - acc: 0.9474 - val_loss: 0.5073 - val_acc: 0.8024\n",
      "Epoch 40/1000\n",
      "58850/58850 [==============================] - 4s 71us/sample - loss: 0.1413 - acc: 0.9490 - val_loss: 0.3702 - val_acc: 0.8688\n",
      "Epoch 41/1000\n",
      "58850/58850 [==============================] - 4s 69us/sample - loss: 0.1363 - acc: 0.9508 - val_loss: 0.3443 - val_acc: 0.8856\n",
      "Epoch 42/1000\n",
      "58850/58850 [==============================] - 4s 70us/sample - loss: 0.1325 - acc: 0.9528 - val_loss: 0.5849 - val_acc: 0.7679\n",
      "Epoch 43/1000\n",
      "58850/58850 [==============================] - 4s 66us/sample - loss: 0.1285 - acc: 0.9544 - val_loss: 0.4788 - val_acc: 0.8234\n",
      "Epoch 44/1000\n",
      "58850/58850 [==============================] - 4s 65us/sample - loss: 0.1299 - acc: 0.9537 - val_loss: 0.4693 - val_acc: 0.8334\n",
      "Epoch 45/1000\n",
      "58850/58850 [==============================] - 4s 66us/sample - loss: 0.1248 - acc: 0.9568 - val_loss: 0.4370 - val_acc: 0.8551\n",
      "Epoch 46/1000\n",
      "58850/58850 [==============================] - 4s 66us/sample - loss: 0.1257 - acc: 0.9564 - val_loss: 0.5791 - val_acc: 0.7853\n",
      "Epoch 47/1000\n",
      "58850/58850 [==============================] - 4s 65us/sample - loss: 0.1184 - acc: 0.9589 - val_loss: 0.6681 - val_acc: 0.7781\n",
      "Epoch 48/1000\n",
      "57600/58850 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9591\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "58850/58850 [==============================] - 7s 116us/sample - loss: 0.1184 - acc: 0.9591 - val_loss: 0.5678 - val_acc: 0.7953\n",
      "Epoch 49/1000\n",
      "58850/58850 [==============================] - 4s 66us/sample - loss: 0.0990 - acc: 0.9662 - val_loss: 0.4456 - val_acc: 0.8544\n",
      "Epoch 50/1000\n",
      "58850/58850 [==============================] - 4s 65us/sample - loss: 0.0968 - acc: 0.9670 - val_loss: 0.7446 - val_acc: 0.7568\n",
      "Epoch 51/1000\n",
      "58850/58850 [==============================] - 4s 66us/sample - loss: 0.0951 - acc: 0.9688 - val_loss: 0.4462 - val_acc: 0.8500\n",
      "Epoch 52/1000\n",
      "58850/58850 [==============================] - 4s 65us/sample - loss: 0.0958 - acc: 0.9673 - val_loss: 0.8298 - val_acc: 0.7323\n",
      "Epoch 53/1000\n",
      "58850/58850 [==============================] - 4s 65us/sample - loss: 0.0928 - acc: 0.9681 - val_loss: 0.4920 - val_acc: 0.8401\n",
      "Epoch 54/1000\n",
      "58400/58850 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9683Restoring model weights from the end of the best epoch.\n",
      "58850/58850 [==============================] - 4s 66us/sample - loss: 0.0932 - acc: 0.9682 - val_loss: 0.7447 - val_acc: 0.7597\n",
      "Epoch 00054: early stopping\n",
      "0.975 0.9393939393939394 0.96\n",
      "['GaCo06', 'GaCo11', 'GaCo16', 'GaPt05', 'GaPt23', 'GaPt28', 'JuCo09', 'JuCo15', 'JuCo16', 'JuCo22', 'JuPt20', 'JuPt24', 'JuPt29', 'SiPt08', 'SiPt17', 'SiPt32']\n",
      "7308/7308 [==============================] - 2s 291us/sample - loss: 0.0961 - acc: 0.9666\n",
      "7308/7308 [==============================] - 2s 293us/sample - loss: 0.7231 - acc: 0.5857\n",
      "Train on 58607 samples, validate on 7308 samples\n",
      "Epoch 1/1000\n",
      "58607/58607 [==============================] - 16s 273us/sample - loss: 0.9843 - acc: 0.6175 - val_loss: 0.5811 - val_acc: 0.7380\n",
      "Epoch 2/1000\n",
      "58607/58607 [==============================] - 4s 67us/sample - loss: 0.6463 - acc: 0.6872 - val_loss: 0.5908 - val_acc: 0.7200\n",
      "Epoch 3/1000\n",
      "58607/58607 [==============================] - 4s 70us/sample - loss: 0.5789 - acc: 0.7232 - val_loss: 0.6916 - val_acc: 0.7284\n",
      "Epoch 4/1000\n",
      "58607/58607 [==============================] - 4s 67us/sample - loss: 0.5358 - acc: 0.7522 - val_loss: 0.6406 - val_acc: 0.7196\n",
      "Epoch 5/1000\n",
      "58607/58607 [==============================] - 4s 74us/sample - loss: 0.5099 - acc: 0.7686 - val_loss: 0.5958 - val_acc: 0.7189\n",
      "Epoch 6/1000\n",
      "58607/58607 [==============================] - 4s 67us/sample - loss: 0.4913 - acc: 0.7832 - val_loss: 0.6371 - val_acc: 0.6730\n",
      "Epoch 7/1000\n",
      "58607/58607 [==============================] - 4s 66us/sample - loss: 0.4750 - acc: 0.7911 - val_loss: 0.6190 - val_acc: 0.7307\n",
      "Epoch 8/1000\n",
      "58607/58607 [==============================] - 4s 67us/sample - loss: 0.4628 - acc: 0.7991 - val_loss: 0.6351 - val_acc: 0.6849\n",
      "Epoch 9/1000\n",
      "58607/58607 [==============================] - 4s 68us/sample - loss: 0.4517 - acc: 0.8071 - val_loss: 0.6031 - val_acc: 0.7306\n",
      "Epoch 10/1000\n",
      "58607/58607 [==============================] - 4s 70us/sample - loss: 0.4390 - acc: 0.8149 - val_loss: 0.5943 - val_acc: 0.7148\n",
      "Epoch 11/1000\n",
      "58607/58607 [==============================] - 4s 66us/sample - loss: 0.4276 - acc: 0.8206 - val_loss: 0.6045 - val_acc: 0.7170\n",
      "Epoch 12/1000\n",
      "58607/58607 [==============================] - 4s 69us/sample - loss: 0.4183 - acc: 0.8250 - val_loss: 0.6041 - val_acc: 0.7492\n",
      "Epoch 13/1000\n",
      "58607/58607 [==============================] - 4s 70us/sample - loss: 0.3979 - acc: 0.8350 - val_loss: 0.6490 - val_acc: 0.7571\n",
      "Epoch 14/1000\n",
      "58607/58607 [==============================] - 4s 68us/sample - loss: 0.3885 - acc: 0.8393 - val_loss: 0.6125 - val_acc: 0.7585\n",
      "Epoch 15/1000\n",
      "57600/58607 [============================>.] - ETA: 0s - loss: 0.3719 - acc: 0.8473\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "58607/58607 [==============================] - 7s 125us/sample - loss: 0.3722 - acc: 0.8472 - val_loss: 0.5944 - val_acc: 0.7689\n",
      "Epoch 16/1000\n",
      "58607/58607 [==============================] - 4s 69us/sample - loss: 0.3385 - acc: 0.8641 - val_loss: 0.5579 - val_acc: 0.7820\n",
      "Epoch 17/1000\n",
      "58607/58607 [==============================] - 4s 66us/sample - loss: 0.3264 - acc: 0.8707 - val_loss: 0.6114 - val_acc: 0.7594\n",
      "Epoch 18/1000\n",
      "58607/58607 [==============================] - 4s 64us/sample - loss: 0.3146 - acc: 0.8765 - val_loss: 0.5725 - val_acc: 0.7835\n",
      "Epoch 19/1000\n",
      "58607/58607 [==============================] - 4s 66us/sample - loss: 0.3046 - acc: 0.8796 - val_loss: 0.6390 - val_acc: 0.7356\n",
      "Epoch 20/1000\n",
      "58607/58607 [==============================] - 4s 66us/sample - loss: 0.2925 - acc: 0.8854 - val_loss: 0.5852 - val_acc: 0.7875\n",
      "Epoch 21/1000\n",
      "58607/58607 [==============================] - 4s 65us/sample - loss: 0.2801 - acc: 0.8915 - val_loss: 0.7240 - val_acc: 0.7091\n",
      "Epoch 22/1000\n",
      "58607/58607 [==============================] - 4s 66us/sample - loss: 0.2722 - acc: 0.8935 - val_loss: 0.6829 - val_acc: 0.7475\n",
      "Epoch 23/1000\n",
      "58607/58607 [==============================] - 4s 67us/sample - loss: 0.2674 - acc: 0.8966 - val_loss: 0.8334 - val_acc: 0.6984\n",
      "Epoch 24/1000\n",
      "58607/58607 [==============================] - 4s 65us/sample - loss: 0.2548 - acc: 0.9010 - val_loss: 0.6889 - val_acc: 0.7573\n",
      "Epoch 25/1000\n",
      "58607/58607 [==============================] - 4s 66us/sample - loss: 0.2461 - acc: 0.9061 - val_loss: 0.7138 - val_acc: 0.7638\n",
      "Epoch 26/1000\n",
      "58607/58607 [==============================] - 4s 65us/sample - loss: 0.2373 - acc: 0.9092 - val_loss: 0.6678 - val_acc: 0.7768\n",
      "Epoch 27/1000\n",
      "58607/58607 [==============================] - 4s 66us/sample - loss: 0.2322 - acc: 0.9116 - val_loss: 0.7837 - val_acc: 0.7536\n",
      "Epoch 28/1000\n",
      "58607/58607 [==============================] - 4s 66us/sample - loss: 0.2217 - acc: 0.9164 - val_loss: 0.9496 - val_acc: 0.6705\n",
      "Epoch 29/1000\n",
      "58607/58607 [==============================] - 4s 67us/sample - loss: 0.2151 - acc: 0.9179 - val_loss: 0.8250 - val_acc: 0.7560\n",
      "Epoch 30/1000\n",
      "58400/58607 [============================>.] - ETA: 0s - loss: 0.2108 - acc: 0.9186\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "58607/58607 [==============================] - 4s 65us/sample - loss: 0.2109 - acc: 0.9186 - val_loss: 0.7160 - val_acc: 0.7597\n",
      "Epoch 31/1000\n",
      "58607/58607 [==============================] - 4s 66us/sample - loss: 0.1953 - acc: 0.9249 - val_loss: 0.7904 - val_acc: 0.7579\n",
      "Epoch 32/1000\n",
      "58607/58607 [==============================] - 4s 66us/sample - loss: 0.1933 - acc: 0.9263 - val_loss: 0.8039 - val_acc: 0.7551\n",
      "Epoch 33/1000\n",
      "58607/58607 [==============================] - 4s 65us/sample - loss: 0.1871 - acc: 0.9286 - val_loss: 0.8333 - val_acc: 0.7318\n",
      "Epoch 34/1000\n",
      "58607/58607 [==============================] - 4s 65us/sample - loss: 0.1865 - acc: 0.9299 - val_loss: 0.7506 - val_acc: 0.7745\n",
      "Epoch 35/1000\n",
      "58607/58607 [==============================] - 4s 66us/sample - loss: 0.1826 - acc: 0.9307 - val_loss: 0.7470 - val_acc: 0.7581\n",
      "Epoch 36/1000\n",
      "57600/58607 [============================>.] - ETA: 0s - loss: 0.1783 - acc: 0.9320Restoring model weights from the end of the best epoch.\n",
      "58607/58607 [==============================] - 4s 65us/sample - loss: 0.1789 - acc: 0.9318 - val_loss: 0.8727 - val_acc: 0.7397\n",
      "Epoch 00036: early stopping\n",
      "0.7692307692307692 0.8055555555555556 0.8771929824561403\n",
      "['GaCo08', 'GaCo22', 'GaPt17', 'GaPt31', 'JuPt01', 'JuPt04', 'JuPt12', 'JuPt17', 'SiCo11', 'SiCo13', 'SiCo17', 'SiCo19', 'SiCo28', 'SiPt05', 'SiPt27', 'SiPt33']\n",
      "5514/5514 [==============================] - 2s 316us/sample - loss: 0.5567 - acc: 0.7231\n",
      "5514/5514 [==============================] - 2s 316us/sample - loss: 0.8262 - acc: 0.5682\n",
      "Train on 60401 samples, validate on 5514 samples\n",
      "Epoch 1/1000\n",
      "60401/60401 [==============================] - 15s 252us/sample - loss: 0.9370 - acc: 0.6327 - val_loss: 0.7032 - val_acc: 0.5841\n",
      "Epoch 2/1000\n",
      "60401/60401 [==============================] - 4s 67us/sample - loss: 0.6391 - acc: 0.6918 - val_loss: 0.7746 - val_acc: 0.6130\n",
      "Epoch 3/1000\n",
      "60401/60401 [==============================] - 4s 66us/sample - loss: 0.5678 - acc: 0.7286 - val_loss: 0.7271 - val_acc: 0.6347\n",
      "Epoch 4/1000\n",
      "60401/60401 [==============================] - 4s 66us/sample - loss: 0.5320 - acc: 0.7514 - val_loss: 0.6789 - val_acc: 0.6672\n",
      "Epoch 5/1000\n",
      "60401/60401 [==============================] - 4s 66us/sample - loss: 0.5022 - acc: 0.7720 - val_loss: 0.9492 - val_acc: 0.6224\n",
      "Epoch 6/1000\n",
      "60401/60401 [==============================] - 4s 65us/sample - loss: 0.4817 - acc: 0.7859 - val_loss: 0.8079 - val_acc: 0.6551\n",
      "Epoch 7/1000\n",
      "60401/60401 [==============================] - 4s 66us/sample - loss: 0.4624 - acc: 0.7999 - val_loss: 0.7437 - val_acc: 0.6569\n",
      "Epoch 8/1000\n",
      "60401/60401 [==============================] - 4s 66us/sample - loss: 0.4454 - acc: 0.8090 - val_loss: 0.7288 - val_acc: 0.6551\n",
      "Epoch 9/1000\n",
      "60401/60401 [==============================] - 4s 66us/sample - loss: 0.4364 - acc: 0.8165 - val_loss: 0.7144 - val_acc: 0.6663\n",
      "Epoch 10/1000\n",
      "60401/60401 [==============================] - 4s 66us/sample - loss: 0.4211 - acc: 0.8228 - val_loss: 0.8994 - val_acc: 0.6242\n",
      "Epoch 11/1000\n",
      "60401/60401 [==============================] - 4s 65us/sample - loss: 0.4091 - acc: 0.8308 - val_loss: 0.8234 - val_acc: 0.6494\n",
      "Epoch 12/1000\n",
      "60401/60401 [==============================] - 4s 65us/sample - loss: 0.3999 - acc: 0.8357 - val_loss: 0.8351 - val_acc: 0.6569\n",
      "Epoch 13/1000\n",
      "60401/60401 [==============================] - 4s 66us/sample - loss: 0.3857 - acc: 0.8432 - val_loss: 0.8657 - val_acc: 0.6427\n",
      "Epoch 14/1000\n",
      "60401/60401 [==============================] - 4s 66us/sample - loss: 0.3733 - acc: 0.8510 - val_loss: 0.9170 - val_acc: 0.6253\n",
      "Epoch 15/1000\n",
      "60401/60401 [==============================] - 4s 65us/sample - loss: 0.3577 - acc: 0.8577 - val_loss: 0.8788 - val_acc: 0.6520\n",
      "Epoch 16/1000\n",
      "60401/60401 [==============================] - 4s 65us/sample - loss: 0.3456 - acc: 0.8610 - val_loss: 0.9309 - val_acc: 0.6371\n",
      "Epoch 17/1000\n",
      "60401/60401 [==============================] - 4s 66us/sample - loss: 0.3288 - acc: 0.8719 - val_loss: 0.9135 - val_acc: 0.6242\n",
      "Epoch 18/1000\n",
      "59200/60401 [============================>.] - ETA: 0s - loss: 0.3138 - acc: 0.8787\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "60401/60401 [==============================] - 7s 122us/sample - loss: 0.3133 - acc: 0.8788 - val_loss: 1.0508 - val_acc: 0.6320\n",
      "Epoch 19/1000\n",
      "60401/60401 [==============================] - 4s 65us/sample - loss: 0.2733 - acc: 0.8964 - val_loss: 0.9982 - val_acc: 0.6407\n",
      "Epoch 20/1000\n",
      "60401/60401 [==============================] - 4s 66us/sample - loss: 0.2601 - acc: 0.9017 - val_loss: 1.2139 - val_acc: 0.6132\n",
      "Epoch 21/1000\n",
      "60401/60401 [==============================] - 4s 65us/sample - loss: 0.2527 - acc: 0.9069 - val_loss: 1.2144 - val_acc: 0.6155\n",
      "Epoch 22/1000\n",
      "60401/60401 [==============================] - 4s 65us/sample - loss: 0.2419 - acc: 0.9106 - val_loss: 1.2777 - val_acc: 0.6124\n",
      "Epoch 23/1000\n",
      "60401/60401 [==============================] - 4s 65us/sample - loss: 0.2336 - acc: 0.9152 - val_loss: 1.2967 - val_acc: 0.5983\n",
      "Epoch 24/1000\n",
      "59200/60401 [============================>.] - ETA: 0s - loss: 0.2275 - acc: 0.9152Restoring model weights from the end of the best epoch.\n",
      "60401/60401 [==============================] - 4s 66us/sample - loss: 0.2277 - acc: 0.9151 - val_loss: 1.2843 - val_acc: 0.6141\n",
      "Epoch 00024: early stopping\n",
      "0.6790123456790124 0.5925925925925926 0.7027027027027027\n",
      "['GaCo10', 'GaCo17', 'GaPt04', 'GaPt15', 'GaPt16', 'JuCo03', 'JuCo19', 'JuPt09', 'SiCo05', 'SiCo26', 'SiCo27', 'SiPt02', 'SiPt13', 'SiPt30', 'SiPt34', 'SiPt40']\n",
      "5973/5973 [==============================] - 2s 301us/sample - loss: 0.3416 - acc: 0.8572\n",
      "5973/5973 [==============================] - 2s 302us/sample - loss: 0.8438 - acc: 0.5028\n",
      "Train on 59942 samples, validate on 5973 samples\n",
      "Epoch 1/1000\n",
      "59942/59942 [==============================] - 15s 251us/sample - loss: 0.9614 - acc: 0.6190 - val_loss: 0.7630 - val_acc: 0.6357\n",
      "Epoch 2/1000\n",
      "59942/59942 [==============================] - 4s 65us/sample - loss: 0.6592 - acc: 0.6760 - val_loss: 0.4880 - val_acc: 0.8401\n",
      "Epoch 3/1000\n",
      "59942/59942 [==============================] - 4s 64us/sample - loss: 0.5910 - acc: 0.7135 - val_loss: 0.4193 - val_acc: 0.8110\n",
      "Epoch 4/1000\n",
      "59942/59942 [==============================] - 4s 64us/sample - loss: 0.5642 - acc: 0.7293 - val_loss: 0.4173 - val_acc: 0.8445\n",
      "Epoch 5/1000\n",
      "59942/59942 [==============================] - 4s 64us/sample - loss: 0.5332 - acc: 0.7540 - val_loss: 0.4323 - val_acc: 0.7901\n",
      "Epoch 6/1000\n",
      "59942/59942 [==============================] - 4s 64us/sample - loss: 0.5160 - acc: 0.7637 - val_loss: 0.3766 - val_acc: 0.8538\n",
      "Epoch 7/1000\n",
      "59942/59942 [==============================] - 4s 67us/sample - loss: 0.4996 - acc: 0.7760 - val_loss: 0.3815 - val_acc: 0.8463\n",
      "Epoch 8/1000\n",
      "59942/59942 [==============================] - 4s 68us/sample - loss: 0.4831 - acc: 0.7851 - val_loss: 0.3658 - val_acc: 0.8528\n",
      "Epoch 9/1000\n",
      "59942/59942 [==============================] - 4s 66us/sample - loss: 0.4744 - acc: 0.7907 - val_loss: 0.3656 - val_acc: 0.8669\n",
      "Epoch 10/1000\n",
      "59942/59942 [==============================] - 4s 65us/sample - loss: 0.4675 - acc: 0.7962 - val_loss: 0.4134 - val_acc: 0.8152\n",
      "Epoch 11/1000\n",
      "59942/59942 [==============================] - 4s 67us/sample - loss: 0.4524 - acc: 0.8045 - val_loss: 0.4184 - val_acc: 0.8192\n",
      "Epoch 12/1000\n",
      "59942/59942 [==============================] - 4s 65us/sample - loss: 0.4440 - acc: 0.8085 - val_loss: 0.3939 - val_acc: 0.8604\n",
      "Epoch 13/1000\n",
      "59942/59942 [==============================] - 4s 66us/sample - loss: 0.4276 - acc: 0.8147 - val_loss: 0.3999 - val_acc: 0.8418\n",
      "Epoch 14/1000\n",
      "59942/59942 [==============================] - 4s 65us/sample - loss: 0.4127 - acc: 0.8239 - val_loss: 0.3909 - val_acc: 0.8585\n",
      "Epoch 15/1000\n",
      "59942/59942 [==============================] - 4s 66us/sample - loss: 0.3918 - acc: 0.8353 - val_loss: 0.4116 - val_acc: 0.8384\n",
      "Epoch 16/1000\n",
      "59942/59942 [==============================] - 4s 65us/sample - loss: 0.3735 - acc: 0.8443 - val_loss: 0.3669 - val_acc: 0.8724\n",
      "Epoch 17/1000\n",
      "59942/59942 [==============================] - 4s 66us/sample - loss: 0.3568 - acc: 0.8508 - val_loss: 0.4245 - val_acc: 0.8277\n",
      "Epoch 18/1000\n",
      "59942/59942 [==============================] - 4s 66us/sample - loss: 0.3386 - acc: 0.8606 - val_loss: 0.3894 - val_acc: 0.8788\n",
      "Epoch 19/1000\n",
      "59942/59942 [==============================] - 4s 65us/sample - loss: 0.3194 - acc: 0.8684 - val_loss: 0.4204 - val_acc: 0.8533\n",
      "Epoch 20/1000\n",
      "59942/59942 [==============================] - 4s 65us/sample - loss: 0.3032 - acc: 0.8763 - val_loss: 0.4706 - val_acc: 0.8366\n",
      "Epoch 21/1000\n",
      "59942/59942 [==============================] - 4s 66us/sample - loss: 0.2864 - acc: 0.8841 - val_loss: 0.4116 - val_acc: 0.8522\n",
      "Epoch 22/1000\n",
      "59942/59942 [==============================] - 4s 67us/sample - loss: 0.2721 - acc: 0.8903 - val_loss: 0.4206 - val_acc: 0.8542\n",
      "Epoch 23/1000\n",
      "58400/59942 [============================>.] - ETA: 0s - loss: 0.2567 - acc: 0.8971\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "59942/59942 [==============================] - 8s 127us/sample - loss: 0.2570 - acc: 0.8972 - val_loss: 0.4186 - val_acc: 0.8617\n",
      "Epoch 24/1000\n",
      "59942/59942 [==============================] - 4s 66us/sample - loss: 0.2257 - acc: 0.9111 - val_loss: 0.3829 - val_acc: 0.8823\n",
      "Epoch 25/1000\n",
      "59942/59942 [==============================] - 4s 65us/sample - loss: 0.2141 - acc: 0.9154 - val_loss: 0.3867 - val_acc: 0.8826\n",
      "Epoch 26/1000\n",
      "59942/59942 [==============================] - 4s 64us/sample - loss: 0.2079 - acc: 0.9177 - val_loss: 0.4085 - val_acc: 0.8719\n",
      "Epoch 27/1000\n",
      "59942/59942 [==============================] - 4s 66us/sample - loss: 0.1999 - acc: 0.9225 - val_loss: 0.3841 - val_acc: 0.8734\n",
      "Epoch 28/1000\n",
      "59942/59942 [==============================] - 4s 64us/sample - loss: 0.1948 - acc: 0.9245 - val_loss: 0.3556 - val_acc: 0.8831\n",
      "Epoch 29/1000\n",
      "59942/59942 [==============================] - 4s 64us/sample - loss: 0.1906 - acc: 0.9262 - val_loss: 0.3340 - val_acc: 0.8848\n",
      "Epoch 30/1000\n",
      "59942/59942 [==============================] - 4s 63us/sample - loss: 0.1832 - acc: 0.9292 - val_loss: 0.4191 - val_acc: 0.8594\n",
      "Epoch 31/1000\n",
      "59942/59942 [==============================] - 4s 63us/sample - loss: 0.1807 - acc: 0.9308 - val_loss: 0.3945 - val_acc: 0.8652\n",
      "Epoch 32/1000\n",
      "59942/59942 [==============================] - 4s 65us/sample - loss: 0.1751 - acc: 0.9340 - val_loss: 0.3394 - val_acc: 0.8800\n",
      "Epoch 33/1000\n",
      "59942/59942 [==============================] - 4s 66us/sample - loss: 0.1697 - acc: 0.9353 - val_loss: 0.4334 - val_acc: 0.8518\n",
      "Epoch 34/1000\n",
      "59942/59942 [==============================] - 4s 66us/sample - loss: 0.1650 - acc: 0.9371 - val_loss: 0.3947 - val_acc: 0.8714\n",
      "Epoch 35/1000\n",
      "59942/59942 [==============================] - 4s 67us/sample - loss: 0.1613 - acc: 0.9380 - val_loss: 0.3566 - val_acc: 0.8826\n",
      "Epoch 36/1000\n",
      "59942/59942 [==============================] - 4s 65us/sample - loss: 0.1591 - acc: 0.9403 - val_loss: 0.5087 - val_acc: 0.8175\n",
      "Epoch 37/1000\n",
      "59942/59942 [==============================] - 4s 66us/sample - loss: 0.1531 - acc: 0.9437 - val_loss: 0.5437 - val_acc: 0.7844\n",
      "Epoch 38/1000\n",
      "59942/59942 [==============================] - 4s 66us/sample - loss: 0.1500 - acc: 0.9452 - val_loss: 0.3558 - val_acc: 0.8778\n",
      "Epoch 39/1000\n",
      "59942/59942 [==============================] - 4s 66us/sample - loss: 0.1464 - acc: 0.9453 - val_loss: 0.3779 - val_acc: 0.8689\n",
      "Epoch 40/1000\n",
      "59942/59942 [==============================] - 4s 66us/sample - loss: 0.1444 - acc: 0.9464 - val_loss: 0.4169 - val_acc: 0.8552\n",
      "Epoch 41/1000\n",
      "59942/59942 [==============================] - 4s 65us/sample - loss: 0.1416 - acc: 0.9483 - val_loss: 0.3681 - val_acc: 0.8848\n",
      "Epoch 42/1000\n",
      "59942/59942 [==============================] - 4s 65us/sample - loss: 0.1398 - acc: 0.9482 - val_loss: 0.4544 - val_acc: 0.8381\n",
      "Epoch 43/1000\n",
      "59200/59942 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.9503\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "59942/59942 [==============================] - 4s 66us/sample - loss: 0.1355 - acc: 0.9503 - val_loss: 0.5192 - val_acc: 0.7979\n",
      "Epoch 44/1000\n",
      "59942/59942 [==============================] - 4s 66us/sample - loss: 0.1287 - acc: 0.9540 - val_loss: 0.4253 - val_acc: 0.8580\n",
      "Epoch 45/1000\n",
      "59942/59942 [==============================] - 4s 65us/sample - loss: 0.1234 - acc: 0.9553 - val_loss: 0.4538 - val_acc: 0.8408\n",
      "Epoch 46/1000\n",
      "59942/59942 [==============================] - 4s 66us/sample - loss: 0.1217 - acc: 0.9567 - val_loss: 0.4023 - val_acc: 0.8569\n",
      "Epoch 47/1000\n",
      "59942/59942 [==============================] - 4s 66us/sample - loss: 0.1223 - acc: 0.9555 - val_loss: 0.4166 - val_acc: 0.8569\n",
      "Epoch 48/1000\n",
      "59942/59942 [==============================] - 4s 66us/sample - loss: 0.1213 - acc: 0.9566 - val_loss: 0.4127 - val_acc: 0.8512\n",
      "Epoch 49/1000\n",
      "58400/59942 [============================>.] - ETA: 0s - loss: 0.1201 - acc: 0.9570Restoring model weights from the end of the best epoch.\n",
      "59942/59942 [==============================] - 4s 67us/sample - loss: 0.1202 - acc: 0.9568 - val_loss: 0.4167 - val_acc: 0.8595\n",
      "Epoch 00049: early stopping\n",
      "0.9235294117647058 0.8888888888888888 0.9142857142857143\n",
      "AUC\t\t 0.8561460928047671 0.0923985562144652\n",
      "Accuracy\t 0.8010571644192334 0.10971407106372097\n",
      "F1Score\t\t 0.8575514832442213 0.08482862454513504\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=10,shuffle=True,random_state=0)\n",
    "gold = []\n",
    "predictions = []\n",
    "AUC = []\n",
    "accuracy = []\n",
    "F1Score = []\n",
    "\n",
    "fold = 0\n",
    "try:\n",
    "    shutil.rmtree(modelstore)\n",
    "except:\n",
    "    pass\n",
    "full_model = multiple_cnn1D()\n",
    "\n",
    "full_model.save_weights('temp.h5')\n",
    "for trainI,testI in kfold.split(patients,labels):\n",
    "    fold+=1\n",
    "    train_patients = [patients[i] for i in trainI]\n",
    "    test_patients = [patients[i] for i in testI]\n",
    "    print (test_patients)\n",
    "    \n",
    "    # Get Walk level data\n",
    "    train = get_from_dict(data,train_patients)\n",
    "    test = get_from_dict(data,test_patients)\n",
    "    \n",
    "    trainSeq = [i[0] for i in train]\n",
    "    trainNum = [i[1].values for i in train]\n",
    "    trainLabel = [i[2] for i in train]\n",
    "\n",
    "    testSeq = [i[0] for i in test]\n",
    "    testNum = [i[1].values for i in test]\n",
    "    testLabel = [i[2] for i in test]\n",
    "    \n",
    "    # Normalize Numerical data\n",
    "    scaler = StandardScaler()\n",
    "    trainNum = scaler.fit_transform(trainNum)\n",
    "    testNum = scaler.transform(testNum)\n",
    "    \n",
    "    # Impute Numerical data\n",
    "    imputer = KNNImputer()\n",
    "    trainNum = imputer.fit_transform(trainNum)\n",
    "    testNum = imputer.transform(testNum)\n",
    "    \n",
    "    # Get Windowed Version of Input\n",
    "    trainSeqWindows, trainNumWindows, trainLabelWindows = window(trainSeq,trainNum,trainLabel) \n",
    "    testSeqWindows, testNumWindows, testLabelWindows = window(testSeq,testNum,testLabel) \n",
    "\n",
    "    # Get Padded Version of Input\n",
    "    trainSeqPadded = pad(trainSeq)\n",
    "    testSeqPadded = pad(testSeq)\n",
    "    \n",
    "    # Define Model\n",
    "#     full_model = multiple_cnn1D()\n",
    "    full_model.evaluate(testSeqWindows,testLabelWindows)\n",
    "    full_model.load_weights(\"temp.h5\")\n",
    "    full_model.evaluate(testSeqWindows,testLabelWindows)\n",
    "    opt = optimizers.RMSprop(lr=0.001)\n",
    "    full_model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    earlystopper = EarlyStopping(monitor=\"val_loss\",mode=\"min\",patience=20, restore_best_weights=True,verbose=1, min_delta=0.01)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\",factor=0.5,mode=\"min\",patience=14,verbose=1,min_lr=0.0000001)\n",
    "    \n",
    "    full_model.fit(trainSeqWindows,trainLabelWindows,batch_size=800,epochs=1000,\n",
    "              validation_data=[testSeqWindows,testLabelWindows],\n",
    "              callbacks=[earlystopper,reduce_lr],verbose=1)\n",
    "    \n",
    "    prob = []\n",
    "    binary = []\n",
    "    for i in range(len(testSeq)):\n",
    "        pred = full_model.predict(window([testSeq[i]],[testNum[i]],[testLabel[i]])[0])\n",
    "        prediction = np.mean(pred)\n",
    "        predictions.append(prediction)\n",
    "        prob.append(prediction)\n",
    "        if prediction>0.5:\n",
    "            binary.append(1)\n",
    "        else:\n",
    "            binary.append(0)\n",
    "        gold.append(testLabel[i])\n",
    "        \n",
    "    AUC.append(roc_auc_score(testLabel,prob))\n",
    "    accuracy.append(accuracy_score(testLabel,binary))\n",
    "    F1Score.append(f1_score(testLabel,binary))\n",
    "    print (AUC[-1],accuracy[-1],F1Score[-1])\n",
    "    # Cleanup\n",
    "#     del full_model\n",
    "#     del model\n",
    "#     break\n",
    "print (\"AUC\\t\\t\",np.mean(AUC),np.std(AUC))\n",
    "print (\"Accuracy\\t\",np.mean(accuracy),np.std(accuracy))\n",
    "print (\"F1Score\\t\\t\",np.mean(F1Score),np.std(F1Score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC\t\t 0.8561460928047671 0.0923985562144652\n",
      "Accuracy\t 0.8010571644192334 0.10971407106372097\n",
      "F1Score\t\t 0.8575514832442213 0.08482862454513504\n"
     ]
    }
   ],
   "source": [
    "print (\"AUC\\t\\t\",np.mean(AUC),np.std(AUC))\n",
    "print (\"Accuracy\\t\",np.mean(accuracy),np.std(accuracy))\n",
    "print (\"F1Score\\t\\t\",np.mean(F1Score),np.std(F1Score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100, 18)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 100)]        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, 100)]        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 100)]        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [(None, 100)]        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 100)]        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [(None, 100)]        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 100)]        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [(None, 100)]        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 100)]        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [(None, 100)]        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 100)]        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [(None, 100)]        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 100)]        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [(None, 100)]        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 100)]        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [(None, 100)]        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 100)]        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [(None, 100)]        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims (TensorF [(None, 100, 1)]     0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_1 (Tenso [(None, 100, 1)]     0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_2 (Tenso [(None, 100, 1)]     0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_3 (Tenso [(None, 100, 1)]     0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_4 (Tenso [(None, 100, 1)]     0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_5 (Tenso [(None, 100, 1)]     0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_6 (Tenso [(None, 100, 1)]     0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_7 (Tenso [(None, 100, 1)]     0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_8 (Tenso [(None, 100, 1)]     0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_9 (Tenso [(None, 100, 1)]     0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_10 (Tens [(None, 100, 1)]     0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_11 (Tens [(None, 100, 1)]     0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_12 (Tens [(None, 100, 1)]     0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_13 (Tens [(None, 100, 1)]     0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_14 (Tens [(None, 100, 1)]     0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_15 (Tens [(None, 100, 1)]     0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_16 (Tens [(None, 100, 1)]     0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_17 (Tens [(None, 100, 1)]     0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "model (Model)                   (None, 50)           19650       tf_op_layer_ExpandDims[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 50)           19650       tf_op_layer_ExpandDims_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, 50)           19650       tf_op_layer_ExpandDims_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 (None, 50)           19650       tf_op_layer_ExpandDims_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "model_4 (Model)                 (None, 50)           19650       tf_op_layer_ExpandDims_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "model_5 (Model)                 (None, 50)           19650       tf_op_layer_ExpandDims_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "model_6 (Model)                 (None, 50)           19650       tf_op_layer_ExpandDims_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "model_7 (Model)                 (None, 50)           19650       tf_op_layer_ExpandDims_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "model_8 (Model)                 (None, 50)           19650       tf_op_layer_ExpandDims_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "model_9 (Model)                 (None, 50)           19650       tf_op_layer_ExpandDims_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "model_10 (Model)                (None, 50)           19650       tf_op_layer_ExpandDims_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "model_11 (Model)                (None, 50)           19650       tf_op_layer_ExpandDims_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "model_12 (Model)                (None, 50)           19650       tf_op_layer_ExpandDims_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "model_13 (Model)                (None, 50)           19650       tf_op_layer_ExpandDims_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "model_14 (Model)                (None, 50)           19650       tf_op_layer_ExpandDims_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "model_15 (Model)                (None, 50)           19650       tf_op_layer_ExpandDims_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "model_16 (Model)                (None, 50)           19650       tf_op_layer_ExpandDims_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "model_17 (Model)                (None, 50)           19650       tf_op_layer_ExpandDims_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 900)          0           model[1][0]                      \n",
      "                                                                 model_1[1][0]                    \n",
      "                                                                 model_2[1][0]                    \n",
      "                                                                 model_3[1][0]                    \n",
      "                                                                 model_4[1][0]                    \n",
      "                                                                 model_5[1][0]                    \n",
      "                                                                 model_6[1][0]                    \n",
      "                                                                 model_7[1][0]                    \n",
      "                                                                 model_8[1][0]                    \n",
      "                                                                 model_9[1][0]                    \n",
      "                                                                 model_10[1][0]                   \n",
      "                                                                 model_11[1][0]                   \n",
      "                                                                 model_12[1][0]                   \n",
      "                                                                 model_13[1][0]                   \n",
      "                                                                 model_14[1][0]                   \n",
      "                                                                 model_15[1][0]                   \n",
      "                                                                 model_16[1][0]                   \n",
      "                                                                 model_17[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 900)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 100)          90100       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 20)           2020        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20)           0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 1)            21          dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 445,841\n",
      "Trainable params: 445,841\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = np.array(gold)\n",
    "pre = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8681938236489233"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(gt,pre)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
